{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Vietnamese Translator using RNNs\n",
    "## Author: Ankit Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133318 133318\n",
      "Khoa học đằng sau một tiêu đề về khí hậu The science behind a climate headline\n"
     ]
    }
   ],
   "source": [
    "# data can downloaded from https://nlp.stanford.edu/projects/nmt/\n",
    "# data is already formatted so that there is space between every two tokens\n",
    "\n",
    "vi_text = open('./datasets/eng-vietnamese/train.vi', encoding='utf-8', errors='strict').read()\n",
    "en_text = open('./datasets/eng-vietnamese/train.en', encoding='utf-8', errors='strict').read()\n",
    "vi_words = vi_text.split()\n",
    "en_words = en_text.split()\n",
    "vi_lines = vi_text.split('\\n')\n",
    "en_lines = en_text.split('\\n')\n",
    "print(len(vi_lines), len(en_lines))\n",
    "print(vi_lines[0], en_lines[0])\n",
    "\n",
    "n_lines = len(vi_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220091\n"
     ]
    }
   ],
   "source": [
    "vi_lines_copy, en_lines_copy = [], []\n",
    "\n",
    "for i in range(n_lines):\n",
    "    if len(en_lines[i].split('.')) == len(vi_lines[i].split('.')):\n",
    "        vi_lines_copy += vi_lines[i].split('.')\n",
    "        en_lines_copy += en_lines[i].split('.')\n",
    "\n",
    "vi_lines, en_lines = vi_lines_copy, en_lines_copy\n",
    "# deleting unnecessary variables \n",
    "del vi_lines_copy, en_lines_copy\n",
    "\n",
    "n_lines = len(vi_lines)\n",
    "print(n_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi_vocab 20000\n",
      "en_vocab 20000\n"
     ]
    }
   ],
   "source": [
    "# build the label mapping\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "vi_vocab = Counter(vi_words) # builds a dict with keys as distict words and their counts in vi_words as vals\n",
    "en_vocab = Counter(en_words) # builds a dict with keys as distict words and their counts in en_words as vals\n",
    "\n",
    "vi_vocab_size = 20000 - 3\n",
    "vi_vocab = [word for (word, count) in vi_vocab.most_common(vi_vocab_size)]\n",
    "# adding a tokens for padding, out-of-vocabulary words, etc\n",
    "vi_vocab = ['<PAD>', '<UNK>', '<GO>'] + vi_vocab\n",
    "vi_vocab_size += 3\n",
    "print('vi_vocab', len(vi_vocab))\n",
    "\n",
    "en_vocab_size = 20000 - 2\n",
    "en_vocab = [word for (word, count) in en_vocab.most_common(en_vocab_size)]\n",
    "# adding a tokens for padding, out-of-vocabulary words, etc\n",
    "en_vocab = ['<PAD>', '<UNK>'] + en_vocab\n",
    "en_vocab_size += 2\n",
    "print('en_vocab', len(en_vocab))\n",
    "\n",
    "vi_label = {}\n",
    "for i in range(len(vi_vocab)):\n",
    "    vi_label[vi_vocab[i]] = i\n",
    "\n",
    "en_label = {}\n",
    "for i in range(len(en_vocab)):\n",
    "    en_label[en_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3311508 2706252\n"
     ]
    }
   ],
   "source": [
    "# encoding the dataset using the above encoder\n",
    "\n",
    "from bisect import bisect_left\n",
    "\n",
    "def search(x, a):\n",
    "    '''return True if x in a''' \n",
    "    #locate the leftmost value exactly equal to x\n",
    "    i = bisect_left(a, x)\n",
    "    return (i != len(a) and a[i] == x)\n",
    "\n",
    "vi_vocab_sorted = sorted(vi_vocab)\n",
    "en_vocab_sorted = sorted(en_vocab)\n",
    "\n",
    "# encoding the words by labels\n",
    "def encode(word_list, label):\n",
    "    labels = []\n",
    "    for word in word_list:\n",
    "        labels.append(label[word])\n",
    "    return np.array(labels).astype('int32')\n",
    "\n",
    "def unk_n_encode(word_list, lang):\n",
    "    # replace missing words by <UNK> and encode\n",
    "    if lang == 'vi':\n",
    "        vocab_sorted = vi_vocab_sorted\n",
    "        label = vi_label\n",
    "    else:\n",
    "        vocab_sorted = en_vocab_sorted\n",
    "        label = en_label\n",
    "    # replacing the out of vocab words in dataset by <UNK>\n",
    "    for i in range(len(word_list)):\n",
    "        if not search(word_list[i], vocab_sorted):\n",
    "            word_list[i] = '<UNK>'\n",
    "    return encode(word_list, label)\n",
    "\n",
    "vi_labels = unk_n_encode(vi_words, 'vi')\n",
    "en_labels = unk_n_encode(en_words, 'en')\n",
    "print(len(vi_labels), len(en_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the datasets in form of a numpy matrix\n",
    "\n",
    "max_en = 0\n",
    "for line in en_lines:\n",
    "    line = line.strip()\n",
    "    if max_en < len(line.split()):\n",
    "        max_en = len(line.split())\n",
    "\n",
    "max_vi = 0\n",
    "for line in vi_lines:\n",
    "    line = line.strip()\n",
    "    if max_vi < len(line.split()):\n",
    "        max_vi = len(line.split())\n",
    "\n",
    "print(max_en, max_vi)\n",
    "\n",
    "vi_lines_mat = np.zeros((len(vi_lines), max_vi), dtype='int32')\n",
    "en_lines_mat = np.zeros((len(en_lines), max_en), dtype='int32')\n",
    "\n",
    "for i, line in enumerate(en_lines):\n",
    "    line = line.strip()\n",
    "    line_labels = unk_n_encode(line.split(), 'en')\n",
    "    for j in range(len(line_labels)):\n",
    "        en_lines_mat[i, j] = line_labels[j]\n",
    "\n",
    "for i, line in enumerate(vi_lines):\n",
    "    line = line.strip()\n",
    "    line_labels = unk_n_encode(line.split(), 'vi')\n",
    "    for j in range(len(line_labels)):\n",
    "        vi_lines_mat[i, j] = line_labels[j]\n",
    "\n",
    "en_label['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember that when constructing decoder inputs we prepend the special GO symbol to the input data. This is done in the get_batch() function in \n",
    "# seq2seq_model.py, which also reverses the input English sentence. Reversing the inputs was shown to improve results for the neural translation model in \n",
    "# Sutskever et al., 2014 (pdf). To put it all together, imagine we have the sentence \"I go.\", tokenized as [\"I\", \"go\", \".\"] as input and the sentence \n",
    "# \"Je vais.\" as output, tokenized [\"Je\", \"vais\", \".\"]. It will be put in the (5, 10) bucket, with encoder inputs representing [PAD PAD \".\" \"go\" \"I\"] and \n",
    "# decoder inputs [GO \"Je\" \"vais\" \".\" EOS PAD PAD PAD PAD PAD]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20) (3, 20)\n",
      "7 2 1684 9748 6 627 1391 146 45 9502 58 6 3805 6 801 614 23 16 90 2221 \n",
      "2 1017 16 11 6 21 647 23 178 12 173 1894 10 244 460 37 172 52 55 58 "
     ]
    }
   ],
   "source": [
    "# reverse the encoder inputs, add <GO> to the decoder inputs \n",
    "\n",
    "n_steps = sequence_length = 20\n",
    "\n",
    "def get_next_batch(epoch, batch, batch_size):\n",
    "    # returns the next batch\n",
    "    np.random.seed(epoch)\n",
    "    line_indices = np.random.permutation(n_lines)[batch*batch_size: (batch+1)*batch_size]\n",
    "    vi_mat, en_mat = vi_lines_mat[line_indices], en_lines_mat[line_indices]\n",
    "    start = min(epoch, max_vi - n_steps - 1, max_en - n_steps - 1)\n",
    "    X_ret, y_ret = en_mat[:, start: start + n_steps], vi_mat[:, start: start + n_steps -1]\n",
    "    #reverse encoder inputs\n",
    "    X_ret = np.flip(X_ret, axis=1)\n",
    "    # add <GO> to decoder inputs\n",
    "    y_ret = np.concatenate((vi_label['<GO>'] * np.ones((batch_size, 1), 'int32'), y_ret), axis=1)\n",
    "    return X_ret.astype('int32'), y_ret.astype('int32')\n",
    "\n",
    "X_temp, y_temp = get_next_batch(0, 0, 3)\n",
    "print(X_temp.shape, y_temp.shape)\n",
    "\n",
    "for i in range(len(y_temp[0])):\n",
    "    print(X_temp[0, i], end=' ')\n",
    "print()\n",
    "for i in range(len(y_temp[0])):\n",
    "    print(y_temp[0, i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing the RNN using encoder-decoder model\n",
    "\n",
    "n_neurons = 400\n",
    "n_layers = 1\n",
    "num_encoder_symbols = len(en_vocab)\n",
    "num_decoder_symbols = len(vi_vocab)\n",
    "embedding_size = 150\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype='int32', shape=[None, n_steps]) # English\n",
    "y = tf.placeholder(dtype='int32', shape=[None, n_steps]) # Vietnamese\n",
    "weights = tf.placeholder(dtype='float32', shape=[None]) # [batch_size*(n_steps - 1)]\n",
    "\n",
    "y_input = y[:, :-1]\n",
    "y_target = y[:, 1:]\n",
    "\n",
    "encoder_inputs = tf.unstack(tf.transpose(X)) # list of 1D tensors\n",
    "decoder_inputs = tf.unstack(tf.transpose(y_input)) # list of 1D tensors\n",
    "\n",
    "lstm_cells = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.elu)\n",
    "              for layer in range(n_layers)]\n",
    "cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "output_seqs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "    encoder_inputs,\n",
    "    decoder_inputs,\n",
    "    cell,\n",
    "    num_encoder_symbols,\n",
    "    num_decoder_symbols,\n",
    "    embedding_size)\n",
    "\n",
    "logits = tf.transpose(tf.unstack(output_seqs), perm=[1, 0, 2])  # [?, n_steps - 1, 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits_flat = tf.reshape(logits, [-1, num_decoder_symbols])\n",
    "y_target_flat = tf.reshape(y_target, [-1])\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_target_flat, \n",
    "                                                          logits=logits_flat\n",
    "                                                         ) # [batch_size*(n_steps-1)]\n",
    "loss = tf.reduce_sum(xentropy)\n",
    "#w_entropy = xentropy * weights\n",
    "#loss = tf.reduce_sum(w_entropy) # instead of reduce_mean(xentropy)\n",
    "\n",
    "learning_rate = tf.placeholder(dtype='float32', shape=None)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# checking for exploding/vanishing grads\n",
    "bottom_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)[1]\n",
    "bottom_grads = optimizer.compute_gradients(loss, var_list=[bottom_vars])\n",
    "grads_norm = tf.norm(bottom_grads, ord=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running a tf session\n",
    "\n",
    "epoch, batch = 0, 0\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 130 loss 270.834 131 132 loss 271.81 133 134 loss 281.535 135 136 loss 282.759 137 138 loss 244.341 139 140 loss 279.327 141 142 loss 262.681 143 144 loss 269.195 145 146 loss 286.826 147 148 loss 273.54 149 150 loss 261.926 151 152 loss 277.532 153 154 loss 278.933 155 156 loss 285.147 157 158 loss 291.245 159 160 loss 287.754 161 162 loss 294.469 163 164 loss 271.15 165 166 loss 286.626 167 168 loss 280.354 169 170 loss 258.536 171 172 loss 275.046 173 174 loss 274.365 175 176 loss 275.493 177 178 loss 284.771 179 180 loss 280.304 181 182 loss 281.318 183 184 loss 275.541 185 186 loss 290.393 187 188 loss 273.735 189 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fe083e45aeb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_lines\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#if batch % 10 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def lr(epoch, batch):\n",
    "    return 0.0001\n",
    "    batch_count = batch + epoch*(n_lines // batch_size)\n",
    "    if batch_count < 100:\n",
    "        return 0.001*(batch_count/100) + 0.01*(1 - batch_count/100)\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "n_epochs = 4\n",
    "batch_size = 100\n",
    "#w = np.array([(0.95)**i for i in range(n_steps - 1)]) # n_steps - 1\n",
    "#w = w / w.sum()\n",
    "#w = list(w) * batch_size\n",
    "#w = np.array(w).astype('float32') # [batch_size*(n_steps-1)]\n",
    "\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    while batch < n_lines // batch_size:\n",
    "        X_batch, y_batch = get_next_batch(epoch, batch, batch_size)\n",
    "        _ , loss_eval = sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch, weights: w, learning_rate: lr(epoch, batch)})\n",
    "        #if batch % 10 == 0:\n",
    "        print(batch, end=' ')\n",
    "        if batch % 2 == 0:\n",
    "            print('loss', loss_eval, end=' ')\n",
    "        #if batch % 200 == 0:\n",
    "        #    print('grads', grads_norm.eval(feed_dict={X: X_batch, y: y_batch, \n",
    "        #                                              learning_rate: lr(epoch, batch)}\n",
    "        #                                  ), end=' ')\n",
    "        batch += 1\n",
    "    print('-----------|epoch|----------', end=' ')\n",
    "    batch = 0\n",
    "    epoch += 1\n",
    "\n",
    "# without dropout it overfits\n",
    "# loss < 270 for it to translate meaningfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./datasets/eng-vietnamese/dec3/en_vi.ckpt\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "\n",
    "saver.save(sess, './datasets/eng-vietnamese/dec3/en_vi.ckpt')\n",
    "#saver.restore(sess, './datasets/eng-vietnamese/dec3/en_vi.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điều này cho phép bạn nghe về vấn đề . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> "
     ]
    }
   ],
   "source": [
    "test_en_sentence = \"This is to inform you about the latest\" \n",
    "# must have <= n_steps words, should have space between words . , ; ' etc\n",
    "\n",
    "test_words = test_en_sentence.strip().split() \n",
    "test_enc = np.zeros((1, n_steps), 'int32')\n",
    "\n",
    "for j, lab in enumerate(unk_n_encode(test_words, 'en')):\n",
    "    test_enc[0, j] = lab\n",
    "test_enc = np.flip(test_enc, axis=1)\n",
    "#print(test_enc)\n",
    "\n",
    "test_dec = np.zeros((1, n_steps), 'int32')\n",
    "test_dec[0, 0] = vi_label['<GO>']\n",
    "\n",
    "for i in range(2*len(test_words)):\n",
    "    logs = logits.eval(feed_dict={X: test_enc, y: test_dec})[0]\n",
    "    lab = np.argmax(logs, axis=1)[i]\n",
    "    print(vi_vocab[lab], end=' ')\n",
    "    test_dec[0, i+1] = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hello, my name is John. chào chào , John là tôi \n",
    "# I am going home . Tôi sẽ mua một nhà .\n",
    "# Yes , I am coming . Vâng , tôi đã bị mất . \n",
    "# Will you be joining us for dinner ? Bạn có thể cho chúng ta ? \n",
    "# I hope it doesn't rain . Tôi hi vọng nó sẽ có thể . \n",
    "# The weather is better today. Nền tảng này được ngày càng tốt hơn .\n",
    "# Can you meet me in one hour . Bạn có thể tôi ở trong một năm .\n",
    "# The police have comfirmed Người ta có những người bị đánh lừa \n",
    "# We were lost in the forest . Chúng tôi đang ở trong khu vực .\n",
    "# This is to inform you about the latest Điều này cho phép bạn nghe về vấn đề ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
